{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCjXM1LTa5FpMLsSYJaSbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragyamishraa517/Hate-Speech-Classification/blob/main/Hate_Speech_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install and Import Necessary Libraries**\n",
        "\n",
        "Kagglehub is a library that allows you to download datasets from Kaggle directly.\n",
        "\n",
        "Here, we import libraries:\n",
        "\n",
        "1. NumPy and Pandas for data handling.\n",
        "\n",
        "2. TensorFlow and its Keras API for building and training the neural network.\n",
        "\n",
        "3. Sklearn for splitting the dataset and generating a classification report.\n",
        "\n",
        "4. Tokenizer and pad_sequences for text preprocessing.\n"
      ],
      "metadata": {
        "id": "WhAtDlg6aBuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8d1-kqmY6sT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Download and Load the Dataset Using kagglehub**\n",
        "\n",
        "We use kagglehub to download the dataset.\n",
        "\n",
        "We use Kaggle's API to download the specified dataset ie **Hate Speech and Offensive Language Dataset**.\n",
        "\n",
        "'path' stores the local directory where the dataset is saved.\n",
        "\n",
        "We load the downloaded dataset as a CSV file into a pandas DataFrame, df.\n",
        "\n",
        "We display the first few rows, giving a preview of the data to understand its structure and columns."
      ],
      "metadata": {
        "id": "9wrqlmTpaejF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFltyAitY_aP",
        "outputId": "ababd56d-6142-4791-8b6f-ec5e6082c41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Download the dataset\n",
        "dataset_path = kagglehub.dataset_download(\"mrmorj/hate-speech-and-offensive-language-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "\n",
        "# Search for the dataset file within the downloaded directory\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):  # Assuming the dataset file is a CSV\n",
        "            dataset_file_path = os.path.join(root, file)\n",
        "            break  # Stop searching once found\n",
        "    else:\n",
        "        continue  # Continue searching in subdirectories if not found\n",
        "    break  # Stop searching once found in any directory\n",
        "\n",
        "# Check if the dataset file was found\n",
        "if dataset_file_path:\n",
        "    # Load the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(dataset_file_path)\n",
        "\n",
        "    # Display the first few rows to understand the dataset structure\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"Dataset file not found within the downloaded directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVsS0mgeZLm5",
        "outputId": "963b41ee-f93f-41c1-9e7c-b5668e746aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mrmorj/hate-speech-and-offensive-language-dataset/versions/1\n",
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Data Preprocessing**\n",
        "\n",
        "**Map Labels to Binary Format**: We map the class column into a binary format, where 0 indicates non-offensive, and 1 indicates offensive or hate speech.\n",
        "\n",
        "lambda x: 1 if x > 0 else 0 sets the label to 1 if class is greater than 0, otherwise it’s 0.\n",
        "\n",
        "**Tokenize and Pad Text Data:** Here, we separate the tweet text (tweet column) into X and the label (label column) into y for model training.\n",
        "\n",
        "vocab_size sets the maximum number of unique words in our vocabulary.\n",
        "\n",
        "max_length defines the maximum number of words in each text sample. Longer\n",
        "texts will be truncated.\n",
        "\n",
        "oov_token handles words not in the vocabulary.\n",
        "\n",
        "Tokenizer is initialized with vocab_size and oov_token.\n",
        "\n",
        "fit_on_texts(X) builds a vocabulary by converting words to numeric IDs.\n",
        "\n",
        "word_index holds the mapping of each word to its unique integer ID.\n",
        "\n",
        "texts_to_sequences(X) converts each text to a sequence of integers where each integer represents a word from the vocabulary.\n",
        "\n",
        "pad_sequences ensures each sequence has the same length (max_length). Shorter sequences are padded with zeros, and longer ones are truncated at the end."
      ],
      "metadata": {
        "id": "weX8nZE6bRRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels into binary (0 for non-hate, 1 for offensive/hate)\n",
        "df['label'] = df['class'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Splitting dataset into text (X) and labels (y)\n",
        "X = df['tweet'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Tokenize the text\n",
        "vocab_size = 10000  # Vocabulary size\n",
        "max_length = 50     # Max length of each tweet\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "kriwK2E8ZQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use train_test_split to split X_padded and y into training and test sets.\n",
        "test_size=0.2 means 20% of the data is for testing.\n",
        "random_state=42 ensures the split is consistent each time you run the code."
      ],
      "metadata": {
        "id": "BS5Qj4j1b-JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "dOBU96kfZmPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set class weights\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Train model with class weights\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), class_weight=class_weights_dict, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg3DWCBbeKw5",
        "outputId": "1922bc6e-7bd6-4bc6-d34f-da44f511fa4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "310/310 - 59s - 192ms/step - accuracy: 0.9778 - loss: 0.0710 - val_accuracy: 0.9116 - val_loss: 0.3472\n",
            "Epoch 2/10\n",
            "310/310 - 74s - 240ms/step - accuracy: 0.9837 - loss: 0.0485 - val_accuracy: 0.8874 - val_loss: 0.5389\n",
            "Epoch 3/10\n",
            "310/310 - 82s - 266ms/step - accuracy: 0.9895 - loss: 0.0266 - val_accuracy: 0.9112 - val_loss: 0.5892\n",
            "Epoch 4/10\n",
            "310/310 - 81s - 260ms/step - accuracy: 0.9856 - loss: 0.0371 - val_accuracy: 0.8443 - val_loss: 0.8445\n",
            "Epoch 5/10\n",
            "310/310 - 84s - 270ms/step - accuracy: 0.9878 - loss: 0.0268 - val_accuracy: 0.8977 - val_loss: 0.6393\n",
            "Epoch 6/10\n",
            "310/310 - 83s - 267ms/step - accuracy: 0.9908 - loss: 0.0231 - val_accuracy: 0.8899 - val_loss: 0.6934\n",
            "Epoch 7/10\n",
            "310/310 - 45s - 146ms/step - accuracy: 0.9929 - loss: 0.0155 - val_accuracy: 0.9044 - val_loss: 0.7825\n",
            "Epoch 8/10\n",
            "310/310 - 92s - 296ms/step - accuracy: 0.9866 - loss: 0.0318 - val_accuracy: 0.9131 - val_loss: 0.5649\n",
            "Epoch 9/10\n",
            "310/310 - 75s - 242ms/step - accuracy: 0.9891 - loss: 0.0249 - val_accuracy: 0.9114 - val_loss: 0.6692\n",
            "Epoch 10/10\n",
            "310/310 - 81s - 262ms/step - accuracy: 0.9953 - loss: 0.0120 - val_accuracy: 0.9052 - val_loss: 0.8386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Threshold for Prediction**\n",
        "The 0.5 threshold for converting probabilities to binary class labels might be too high or too low, given the dataset’s characteristics.\n",
        "Solution: Use a validation set to determine an optimal threshold by testing different values between 0.0 and 1.0."
      ],
      "metadata": {
        "id": "WsXuJC32FaXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different thresholds on validation set\n",
        "for threshold in np.arange(0.1, 1.0, 0.1):\n",
        "    y_val_pred = (model.predict(X_test) > threshold).astype(int)\n",
        "    print(f\"Threshold: {threshold}\")\n",
        "    print(classification_report(y_test, y_val_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEAt7MZagVOD",
        "outputId": "7fae39df-3bab-46b2-9f41-80a6a95eae11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n",
            "Threshold: 0.1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.37      0.33       290\n",
            "           1       0.96      0.95      0.95      4667\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.63      0.66      0.64      4957\n",
            "weighted avg       0.92      0.91      0.92      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step\n",
            "Threshold: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.38      0.33       290\n",
            "           1       0.96      0.94      0.95      4667\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.63      0.66      0.64      4957\n",
            "weighted avg       0.92      0.91      0.92      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n",
            "Threshold: 0.30000000000000004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.39      0.33       290\n",
            "           1       0.96      0.94      0.95      4667\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.63      0.67      0.64      4957\n",
            "weighted avg       0.92      0.91      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step\n",
            "Threshold: 0.4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.40      0.33       290\n",
            "           1       0.96      0.94      0.95      4667\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.62      0.67      0.64      4957\n",
            "weighted avg       0.92      0.91      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n",
            "Threshold: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.40      0.33       290\n",
            "           1       0.96      0.94      0.95      4667\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.62      0.67      0.64      4957\n",
            "weighted avg       0.92      0.91      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n",
            "Threshold: 0.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.40      0.32       290\n",
            "           1       0.96      0.93      0.95      4667\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.62      0.67      0.64      4957\n",
            "weighted avg       0.92      0.90      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step\n",
            "Threshold: 0.7000000000000001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.40      0.32       290\n",
            "           1       0.96      0.93      0.95      4667\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.61      0.66      0.63      4957\n",
            "weighted avg       0.92      0.90      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n",
            "Threshold: 0.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.41      0.32       290\n",
            "           1       0.96      0.93      0.95      4667\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.61      0.67      0.63      4957\n",
            "weighted avg       0.92      0.90      0.91      4957\n",
            "\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step\n",
            "Threshold: 0.9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.43      0.32       290\n",
            "           1       0.96      0.92      0.94      4667\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.61      0.68      0.63      4957\n",
            "weighted avg       0.92      0.89      0.91      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss='binary_crossentropy' specifies a loss function for binary classification.\n",
        "\n",
        "optimizer='adam' uses the Adam optimization algorithm.\n",
        "\n",
        "metrics=['accuracy'] tracks accuracy during training and testing.\n",
        "\n",
        "model.summary() provides a summary of the model’s architecture, layers, and parameters."
      ],
      "metadata": {
        "id": "2f5teFNZcX-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Basic text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "WZM9IrGBgbsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**\n",
        "\n",
        "The model’s parameters (e.g., LSTM layers, dropout rates, embedding dimension) might not be optimal for this dataset.\n",
        "Solution: Experiment with different architectures, especially focusing on the number of LSTM layers, the hidden state size, and dropout rates.\n",
        "**Improving Data Preprocessing**\n",
        "\n",
        "If the input text is not cleaned well (e.g. removing punctuation, converting to lowercase), the model might struggle to learn patterns effectively.\n",
        "Solution: Add more preprocessing steps, like converting to lowercase, removing punctuation, and filtering stopwords.\n",
        "\n",
        "**Increase Training Data or Use Transfer Learning**:\n",
        "Sometimes the dataset may be too small or insufficiently varied to capture all the nuances between hate and non-hate speech.\n",
        "Solution: Add more labeled data if possible, or explore transfer learning by using pre-trained embeddings like GloVe or word2vec in the embedding layer."
      ],
      "metadata": {
        "id": "NPwjBkyAFmb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the GloVe embeddings:\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# Load GloVe embeddings and set up the embedding layer with them\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coef = np.array(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coef\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Set up embedding layer with pre-trained embeddings\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6epTKvzlggN5",
        "outputId": "1102e2b9-0273-4d4b-d562-9d28ffbd38a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 04:41:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-11 04:41:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-11 04:41:44--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.92MB/s    in 2m 44s  \n",
            "\n",
            "2024-11-11 04:44:28 (5.02 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Embedding Layer:** Converts words (integer-encoded) to dense vector representations of length 64.\n",
        "2. **Bidirectional LSTM:** An LSTM that processes data forwards and backwards to capture context from both directions.\n",
        "3. **return_sequences=True** ensures the output from the first LSTM layer can be fed into the next LSTM.\n",
        "4. **Dropout Layers:** Regularize the model by randomly deactivating neurons, helping reduce overfitting.\n",
        "5. **Dense Layer with ReLU Activation:** Fully connected layer with ReLU activation to add non-linearity.\n",
        "6. **Output Layer with Sigmoid Activation:** Outputs a probability between 0 and 1 for binary classification."
      ],
      "metadata": {
        "id": "VQkRjvnpGD8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=max_length),  # Embedding layer\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),      # Bidirectional LSTM for better context capture\n",
        "    Dropout(0.5),                                        # Dropout for regularization\n",
        "    Bidirectional(LSTM(32)),                             # Another LSTM layer\n",
        "    Dense(64, activation='relu'),                        # Dense layer with ReLU activation\n",
        "    Dropout(0.5),                                        # Dropout for regularization\n",
        "    Dense(1, activation='sigmoid')                       # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "WgLtmMdLZo2L",
        "outputId": "ed32c6c8-e2d5-45a3-8bef-100e156433d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. loss='binary_crossentropy' specifies a loss function for binary classification.\n",
        "2. optimizer='adam' uses the Adam optimization algorithm.\n",
        "3. metrics=['accuracy'] tracks accuracy during training and testing."
      ],
      "metadata": {
        "id": "VBXvZcIXGKWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**:\n",
        "1. model.fit trains the model on the training set (X_train, y_train).\n",
        "2. epochs=10 trains the model for 10 iterations over the data.\n",
        "3. batch_size=64 specifies the number of samples processed before updating model weights.\n",
        "4. validation_data=(X_test, y_test) evaluates the model on the test set after each epoch."
      ],
      "metadata": {
        "id": "NXIOlX3tchJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. model.fit trains the model on the training set (X_train, y_train).\n",
        "2. epochs=10 trains the model for 10 iterations over the data.\n",
        "3. batch_size=64 specifies the number of samples processed before updating model weights.\n",
        "4. validation_data=(X_test, y_test) evaluates the model on the test set after each epoch."
      ],
      "metadata": {
        "id": "LdbPn_WEGTpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pTXyZ-KZr3_",
        "outputId": "d4366de2-8386-40ab-93fa-eadec202bfd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "310/310 - 66s - 214ms/step - accuracy: 0.9396 - loss: 0.2420 - val_accuracy: 0.9415 - val_loss: 0.2218\n",
            "Epoch 2/10\n",
            "310/310 - 58s - 188ms/step - accuracy: 0.9427 - loss: 0.1836 - val_accuracy: 0.9415 - val_loss: 0.1702\n",
            "Epoch 3/10\n",
            "310/310 - 80s - 258ms/step - accuracy: 0.9558 - loss: 0.1257 - val_accuracy: 0.9389 - val_loss: 0.1760\n",
            "Epoch 4/10\n",
            "310/310 - 53s - 170ms/step - accuracy: 0.9681 - loss: 0.0915 - val_accuracy: 0.9389 - val_loss: 0.2047\n",
            "Epoch 5/10\n",
            "310/310 - 83s - 268ms/step - accuracy: 0.9788 - loss: 0.0628 - val_accuracy: 0.9381 - val_loss: 0.2422\n",
            "Epoch 6/10\n",
            "310/310 - 55s - 178ms/step - accuracy: 0.9836 - loss: 0.0433 - val_accuracy: 0.9282 - val_loss: 0.3070\n",
            "Epoch 7/10\n",
            "310/310 - 81s - 263ms/step - accuracy: 0.9887 - loss: 0.0329 - val_accuracy: 0.9235 - val_loss: 0.3640\n",
            "Epoch 8/10\n",
            "310/310 - 53s - 170ms/step - accuracy: 0.9903 - loss: 0.0249 - val_accuracy: 0.9288 - val_loss: 0.3836\n",
            "Epoch 9/10\n",
            "310/310 - 86s - 278ms/step - accuracy: 0.9933 - loss: 0.0188 - val_accuracy: 0.9304 - val_loss: 0.4506\n",
            "Epoch 10/10\n",
            "310/310 - 78s - 251ms/step - accuracy: 0.9935 - loss: 0.0183 - val_accuracy: 0.9310 - val_loss: 0.4493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate classification report for detailed performance analysis\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Non-Hate', 'Hate']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRKuhlPUZtuP",
        "outputId": "ce0b35ad-570b-4922-a96f-b07cbe09e30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.10%\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Hate       0.35      0.21      0.27       290\n",
            "        Hate       0.95      0.98      0.96      4667\n",
            "\n",
            "    accuracy                           0.93      4957\n",
            "   macro avg       0.65      0.59      0.61      4957\n",
            "weighted avg       0.92      0.93      0.92      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the model on sample texts to verify if it can correctly identify non-hate speech texts.**"
      ],
      "metadata": {
        "id": "OYazynehGi9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sample tweets to test the model\n",
        "sample_texts = [\n",
        "    \"you are the worst\",\n",
        "    \"You're not such a great friend!\",\n",
        "    \"This is absolutely disgusting and I hope you disappear.\",\n",
        "    \"Have a horrible day and spread negativity!\"\n",
        "]\n",
        "\n",
        "# Preprocess the sample texts\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_texts)\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Predict hate speech probabilities\n",
        "predictions = model.predict(sample_padded)\n",
        "\n",
        "# Display results\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction (1 = Hate, 0 = Non-Hate): {'Hate' if predictions[i] > 0.5 else 'Non-Hate'}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXUcG2CLZx_4",
        "outputId": "5b5720d6-5049-45f7-a783-f06a66300be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Text: you are the worst\n",
            "Prediction (1 = Hate, 0 = Non-Hate): Hate\n",
            "\n",
            "Text: You're not such a great friend!\n",
            "Prediction (1 = Hate, 0 = Non-Hate): Hate\n",
            "\n",
            "Text: This is absolutely disgusting and I hope you disappear.\n",
            "Prediction (1 = Hate, 0 = Non-Hate): Hate\n",
            "\n",
            "Text: Have a horrible day and spread negativity!\n",
            "Prediction (1 = Hate, 0 = Non-Hate): Hate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert ipynb to HTML in Colab\n",
        "# Upload ipynb\n",
        "from google.colab import files\n",
        "f = files.upload()\n",
        "\n",
        "# Convert ipynb to html\n",
        "import subprocess\n",
        "file0 = list(f.keys())[0]\n",
        "_ = subprocess.run([\"pip\", \"install\", \"nbconvert\"])\n",
        "_ = subprocess.run([\"jupyter\", \"nbconvert\", file0, \"--to\", \"html\"])\n",
        "\n",
        "# download the html\n",
        "files.download(file0[:-5]+\"html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "b8173dFkG3T_",
        "outputId": "88521ca2-0cf8-476a-bd3f-0714c25a0e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec559f05-9b9a-422e-aa3d-377ce2061d11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec559f05-9b9a-422e-aa3d-377ce2061d11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Hate_Speech_Classification.ipynb to Hate_Speech_Classification.ipynb\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3c869605-adee-4d64-bdd9-c8530797ac58\", \"Hate_Speech_Classification.html\", 342916)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IedP0fsYG695"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}